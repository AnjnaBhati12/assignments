{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The code provides fesility to extract the important keywords from a given text story. \n",
    "\n",
    "# Steps I have taken \n",
    "\n",
    "# 1) cleaning process (Removing unnecessary things, punctuations, html tags etc) and removing redundant entries. \n",
    "# 2) Removing the stopping words. For example, a, an, the etc. \n",
    "# 3) Tokenize the stories using nltk framework. \n",
    "# 4) First collected all those keywords mentioned in \" \" form. Moreover, taken sentences as well in \" \" form. \n",
    "# 5) Now comes the crucial part, for giving importance weight to a keyword. I used \"tf idf algorithm\" - https://en.wikipedia.org/wiki/Tf%E2%80%93idf \n",
    "# 6) The proposed method requires two values a) tf -term frequency  and b) idf- inverse document frequency. It uses these to formulate a statistic tf-idf, based on that I gives importance to the keywords. \n",
    "\n",
    " \n",
    "# The code supports functionality for english language only. There are certain functions to support hindi language too but, it's not complete yet, support for hindi language is in process.\n",
    "      \n",
    "# Written by Maunil Vyas - vyasmaunil33@gmail.com      \n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import operator\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import math\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    " \n",
    "\n",
    "def term_frequency(index, data_set):\n",
    "    dic = {}\n",
    "    \n",
    "    for i in data_set[index]:\n",
    "    \tdic[i] = 0\n",
    "    \n",
    "    for i in data_set[index]:\n",
    "    \tif (dic[i] == 0):\n",
    "    \t    dic[i] = float(1/len(data_set[index]))\n",
    "    \telse: \n",
    "    \t    dic[i] = dic[i] + float(1/len(data_set[index]))    \t\n",
    "    \t\n",
    "    return dic\n",
    "    \n",
    "def n_containing(word, data_set):\n",
    "    counter = 0\n",
    "    for i in data_set:\n",
    "    \tif word in i:\n",
    "    \t   counter = counter + 1\n",
    "    \n",
    "    return counter\n",
    "    \t\n",
    "def inverse_term_frequency(word, eng_doc, data_set):\n",
    "    return math.log(eng_doc / (1 + n_containing(word, data_set)))\n",
    "\n",
    "def tfidf_score(index, eng_doc, data_set):\n",
    "    dic = term_frequency(index, data_set)\n",
    "    #print dic\n",
    "    score_dic = {}\n",
    "    \n",
    "    for i in dic:\n",
    "    \tscore_dic[i] = dic[i]*inverse_term_frequency(i, eng_doc, data_set)\n",
    "\t\n",
    "    return score_dic\n",
    "    \t\t\n",
    "def read_and_clean(filename):\n",
    "    data = []\n",
    "    eng_doc = 0\n",
    "    hi_doc = 0\n",
    "    file_data = csv.reader(open(filename),skipinitialspace=True)\n",
    "    sortedlist = sorted(file_data, key=lambda row: row[0])\n",
    "    \n",
    "    #Removing empty data\n",
    "    for line in sortedlist:\n",
    "        if (line[3] != \"\" and line[1]==\"english\"):\n",
    "\t\tdata.append(line)\n",
    "\t\teng_doc = eng_doc + 1 \n",
    "\telif (line[3] != \"\" and line[1]==\"hindi\"):\n",
    "\t\ttemp = [line[0],line[1],unicode(line[2],\"utf-8\"),unicode(line[3],\"utf-8\"),line[4]]\n",
    "\t\tdata.append(temp)\n",
    "\t\thi_doc = hi_doc + 1\n",
    "\t\t\n",
    "    #Removing Redudnant data with same id number \n",
    "    data1 = []\n",
    "    for i in range(len(data)-1):\n",
    "    \tif data[i][0]!= data[i+1][0]:\n",
    "    \t\tdata1.append(data[i][:])\n",
    "    \n",
    "    if data[len(data)-1][0]!= data[len(data)-1][0]:\n",
    "    \tdata1.append(data[len(data)-1][:])\n",
    "    \t\t\t   \t      \t    \t         \t\n",
    "    return data1,hi_doc,eng_doc\n",
    "\n",
    "def remove_common_words(story,stop_words):\n",
    "    filtered_story = []\n",
    "    \n",
    "    for w in story:\n",
    "    \tword = w.split(\" \")\n",
    "    \tdump = []\t\t\n",
    "    \tfor k in word:\n",
    "    \t    if k not in stop_words:\n",
    "            \tdump.append(k)\n",
    "        str1 = ' '.join(dump)    \t\n",
    "\tfiltered_story.append(str1)\t    \t\n",
    "    return filtered_story\n",
    "\n",
    "def process_the_story(story):\n",
    "    #print story\n",
    "    story = story.replace(\"</p>\",\"\").replace(\"<p>\",\"\").replace(\"\\n\",\"\").replace(\"\\s\",\"\").replace(\"</strong>\",\"\").replace(\":\",\"\")  \n",
    "    #story = story.split(\":\")\n",
    "    story = story.split(\"<br/><center>\")\t\n",
    "    story = story[0].split(\"<em>\")\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    filtered_story = remove_common_words(story,english_stop_words)\n",
    "    filtered_story = ''.join(filtered_story)\n",
    "    filtered_story = filtered_story.lower()\n",
    "    return filtered_story\n",
    "\n",
    "def write_in_csv(data,filename):\n",
    "    f = open(filename, 'w')\n",
    "    with f:\n",
    "    \twriter = csv.writer(f)\n",
    "    \n",
    "    \tfor row in data:\n",
    "        \twriter.writerow(row)\n",
    "    \n",
    "            \n",
    "def tockens(story):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(story)\t\n",
    "\t\n",
    "    \n",
    "def important_keywords(story):\n",
    "    try:\n",
    "    \tfound = (re.search('\\\"(.+?)\\\"', story).group(1))    \t\n",
    "    except AttributeError:\n",
    "    \tfound = \"\"  \n",
    "    \n",
    "    return found\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print \"Code is working!\"\n",
    "    data,hi_doc,eng_doc = read_and_clean(\"articles.csv\")\n",
    "    \n",
    "    clean_data = []\n",
    "    \n",
    "    #dic = term_frequency(data[0][0],clean_data)\n",
    "    \n",
    "    for i in range(eng_doc):\n",
    "    \tclean_data.append(tockens(process_the_story(data[i][3])))\n",
    "    \n",
    "    keys = []\n",
    "    for i in range(eng_doc):\n",
    "    \t    key = []\n",
    "    \t    print i\n",
    "    \t    flag = 0\n",
    "\t    key.append(data[i][2])\n",
    "\t    score = tfidf_score(i, eng_doc, clean_data)\n",
    "    \t    sorted_x = sorted(score.items(), key=operator.itemgetter(1),reverse=True)\t\n",
    "     \t    \n",
    "     \t    story = process_the_story(data[i][3])\n",
    "    \t    found = important_keywords(story)\n",
    "    \t    #imp_keys = []\n",
    "            if (found!=\"\"):\n",
    "    \t        found = \"\\\"\" +found +  \"\\\"\"\n",
    "            while (found!=\"\"):\n",
    "                #flag = 1\n",
    "    \t\tkey.append(important_keywords(story))\n",
    "    \t\tstory = story.replace(found,\"\")\n",
    "    \t\tfound = important_keywords(story)\n",
    "    \t\tif (found!=\"\"):\n",
    "    \t\t\tfound = \"\\\"\" +found +  \"\\\"\"\n",
    "    \t\t\t\n",
    "    \t    #if (flag==1):\n",
    "\t    #\t    key.append(imp_keys)\n",
    "    \t    \n",
    "    \t    for l in range(len(sorted_x)):\n",
    "    \t    \tkey.append(sorted_x[l][0])\n",
    "    \t     \n",
    "    \t    keys.append(key) \n",
    "    \n",
    "    write_in_csv(keys,\"Key_Extraction.csv\")\t\n",
    " \t\t    \n",
    "    #print keys \t\t    \n",
    "    #Writing in CSV \t     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #explicit keywords -> Those in \" \"\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
